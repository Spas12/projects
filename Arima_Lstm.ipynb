{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86004fee-3962-4282-bdfc-b1c80e8fba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "\n",
    "class ARIMA_LSTM:\n",
    "    def __init__(self, ts, exog=None, lstm_params=None):\n",
    "        \"\"\"\n",
    "        Initialize with time series data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts : pd.Series\n",
    "            Time series data.\n",
    "        exog : pd.DataFrame, optional\n",
    "            Exogenous variables for ARIMA, by default None.\n",
    "        lstm_params : dict, optional\n",
    "            Dictionary with LSTM hyperparameters.\n",
    "        \"\"\"\n",
    "        self.data = ts\n",
    "        self.exog = exog\n",
    "        self.arima_model = None\n",
    "        self.lstm_model = None\n",
    "        self.arima_residuals = None\n",
    "        self.sequence_length = None\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Default LSTM parameters\n",
    "        self.lstm_params = lstm_params or {\n",
    "            \"input_size\": 1,\n",
    "            \"hidden_size\": 50,\n",
    "            \"num_layers\": 1,\n",
    "            \"output_size\": 1\n",
    "        }\n",
    "\n",
    "    def fit_arima(self, order=(1, 1, 1), seasonal=(0, 0, 0, 0), trend='c'):\n",
    "        \"\"\"Fits an ARIMA model and stores residuals.\"\"\"\n",
    "        model = sm.tsa.statespace.SARIMAX(self.data, self.exog, seasonal_order=seasonal, trend=trend, order=order)\n",
    "        self.arima_model = model.fit()\n",
    "        self.arima_residuals = self.arima_model.resid\n",
    "\n",
    "    def prepare_lstm_data(self):\n",
    "        \"\"\"\n",
    "        Prepares data for LSTM training from ARIMA residuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int, optional\n",
    "            Length of input sequences, by default 10\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader\n",
    "            PyTorch DataLoader with formatted sequences and targets\n",
    "        \"\"\"\n",
    "        if self.arima_residuals is None:\n",
    "            raise ValueError(\"Fit ARIMA model first to generate residuals\")\n",
    "\n",
    "        # Convert residuals to numpy array\n",
    "        residuals = self.arima_residuals.values.reshape(-1, 1)\n",
    "\n",
    "        # Create sequences and targets\n",
    "        X, y = [], []\n",
    "        for i in range(len(residuals) - self.sequence_length):\n",
    "            X.append(residuals[i:i + self.sequence_length])\n",
    "            y.append(residuals[i + self.sequence_length])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(np.array(y), dtype=torch.float32)\n",
    "\n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def fit_lstm(self, epochs=20, lr=0.001, seq_length=10):\n",
    "        \"\"\"\n",
    "        Fits an LSTM model on the residuals.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataloader : DataLoader\n",
    "            Dataloader for LSTM training.\n",
    "        epochs : int, optional\n",
    "            Number of training epochs, by default 20.\n",
    "        lr : float, optional\n",
    "            Learning rate, by default 0.001.\n",
    "        \"\"\"\n",
    "        self.sequence_length = seq_length\n",
    "        self.lstm_model = LSTMModel(**self.lstm_params).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.lstm_model.parameters(), lr=lr)\n",
    "\n",
    "        dataloader = self.prepare_lstm_data()\n",
    "        self.lstm_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.lstm_model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"\n",
    "        Forecasts mean (ARIMA) and residuals (LSTM).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, optional\n",
    "            Number of steps to forecast, by default 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            ARIMA forecast.\n",
    "        pd.Series\n",
    "            LSTM residual forecast.\n",
    "        \"\"\"\n",
    "        if self.arima_model is None or self.lstm_model is None:\n",
    "            raise ValueError(\"Fit both ARIMA and LSTM before predicting.\")\n",
    "\n",
    "        arima_forecast = self.arima_model.forecast(steps=steps).values\n",
    "\n",
    "        # Prepare residuals as LSTM input\n",
    "        recent_residuals = self.arima_residuals[-self.sequence_length:].values.reshape(-1, 1)\n",
    "        residuals_tensor = torch.tensor(recent_residuals, dtype=torch.float32).unsqueeze(\n",
    "            0)  # Shape: (1, self.sequence_length, 1)\n",
    "\n",
    "        self.lstm_model.eval()\n",
    "        lstm_forecast = []\n",
    "\n",
    "        # For multi-step forecasting\n",
    "        with torch.no_grad():\n",
    "            current_sequence = residuals_tensor.to(self.device)\n",
    "\n",
    "            for _ in range(steps):\n",
    "                # Get prediction for next step\n",
    "                next_pred = self.lstm_model(current_sequence).cpu().numpy().flatten()[0]\n",
    "                lstm_forecast.append(next_pred)\n",
    "\n",
    "                # Update sequence for next prediction (rolling window approach)\n",
    "                # Remove oldest value and append the prediction\n",
    "                current_sequence = torch.cat([\n",
    "                    current_sequence[:, 1:, :],\n",
    "                    torch.tensor([[[next_pred]]], dtype=torch.float32).to(self.device)\n",
    "                ], dim=1)\n",
    "\n",
    "        forecast_index = pd.bdate_range(start=self.data.index[-1] + pd.tseries.offsets.BusinessDay(1), periods=steps)\n",
    "\n",
    "        return pd.Series(arima_forecast, index=forecast_index), pd.Series(lstm_forecast, index=forecast_index)\n",
    "\n",
    "    def plot_results(self, steps=10):\n",
    "        \"\"\"Plots historical data and recent predictions using `predict`.\"\"\"\n",
    "        if self.arima_model is None or self.lstm_model is None:\n",
    "            raise ValueError(\"Fit both ARIMA and LSTM before plotting.\")\n",
    "\n",
    "        last_index = len(self.data) - steps\n",
    "        arima_predicted = self.arima_model.predict(start=last_index, end=len(self.data) - 1)\n",
    "        lstm_predicted = self.arima_residuals[-steps:]  # Approximation\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.data, label='Actual Data', color='black')\n",
    "        plt.plot(arima_predicted, label='ARIMA Predicted', linestyle='dashed', color='blue')\n",
    "        plt.fill_between(arima_predicted.index,\n",
    "                         arima_predicted - lstm_predicted,\n",
    "                         arima_predicted + lstm_predicted,\n",
    "                         color='gray', alpha=0.3, label='Residual Variability')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(f\"ARIMA+LSTM Predictions (Last {steps} Steps)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499b586-b4ab-4f76-b57b-ecb21fed4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"SP.csv\", index_col=\"Date\", parse_dates=True)[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745818c-0515-4e1a-a225-449554195893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA_LSTM(dat['Adj Close'])\n",
    "model.fit_arima()\n",
    "model.fit_lstm(200, 0.01, 60)\n",
    "arima, lstm = model.predict(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41688ec6-b8a6-47f6-81d8-67c5bd2adfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_results(steps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsenv)",
   "language": "python",
   "name": "tsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
